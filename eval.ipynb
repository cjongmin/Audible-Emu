{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers.utils import logging as hf_transformers_logging\n",
    "import logging\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 불필요한 warning 메세지 출력하지 않기 위한 작업\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument.*\", category=UserWarning)\n",
    "hf_transformers_logging.set_verbosity_error()\n",
    "logging.getLogger(\"huggingface_hub.file_download\").setLevel(logging.ERROR)\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, CLIPImageProcessor\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torchvision.transforms as T # torchvision 임포트 추가\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pretrained_model.CLAP import load_clap_model, get_clap_intermediate_patch_embeddings\n",
    "from model.projector import Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    \"emu2\": {\n",
    "        \"name_or_path\": \"BAAI/Emu2\",\n",
    "        \"snapshot_path\": \"/home/jongmin/.cache/huggingface/hub/models--BAAI--Emu2/snapshots/fa835ec101e52da5e081695107e1ddd3c7c4d88a\"\n",
    "    },\n",
    "    \"emu2chat\": {\n",
    "        \"name_or_path\": \"BAAI/Emu2-Chat\",\n",
    "        \"snapshot_path\": \"/home/jongmin/.cache/huggingface/hub/models--BAAI--Emu2-Chat/snapshots/20ea30b04f8fee599cf97535e655c200df728501\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Function to Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_type, projector_checkpoint_path, clap_checkpoint_path):\n",
    "    print(f\"'{model_type}' 모델 로딩 시작...\")\n",
    "    \n",
    "    if model_type not in MODEL_CONFIGS:\n",
    "        raise ValueError(f\"지원되지 않는 모델 타입입니다: {model_type}. 사용 가능: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "    selected_model_config = MODEL_CONFIGS[model_type]\n",
    "    model_name_or_path = selected_model_config[\"name_or_path\"]\n",
    "    snapshot_path = selected_model_config[\"snapshot_path\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    print(f\"Tokenizer ({model_name_or_path}) 로드 완료.\")\n",
    "\n",
    "    with init_empty_weights():\n",
    "        emu_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    print(f\"Emu 모델 ({model_name_or_path}) 구조 로드 완료.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        #num_gpus = torch.cuda.device_count()\n",
    "        #max_memory_setting = {i: '20GiB' for i in range(num_gpus)}\n",
    "        max_memory_setting = {0: '20GiB', 1: '20GiB', 2: '20GiB', 3: '20GiB'} # GPU 메모리 설정 (0,1,2,3번 GPU에 20GiB 할당)\n",
    "        #max_memory_setting = {4: '20GiB', 5: '20GiB', 6: '20GiB', 7: '20GiB'} # GPU 메모리 설정 (4,5,6,7번 GPU에 20GiB 할당)\n",
    "        device_map = infer_auto_device_map(emu_model, max_memory=max_memory_setting, no_split_module_classes=['Block','LlamaDecoderLayer'])\n",
    "        # lm_head 디바이스 설정 (이전 코드에서 개선된 로직 사용 또는 단순화)\n",
    "        main_device_idx_for_lm_head = 0\n",
    "        if device_map:\n",
    "            try: \n",
    "                main_device_idx_for_lm_head = device_map.get(max(device_map, key=lambda k: sum(1 for v in device_map.values() if v == device_map[k])), 0) \n",
    "                if not isinstance(main_device_idx_for_lm_head, int): main_device_idx_for_lm_head = 0 \n",
    "            except: \n",
    "                 main_device_idx_for_lm_head = 0\n",
    "        \n",
    "        lm_head_assigned = False\n",
    "        potential_keys = [\"model.decoder.lm.lm_head\", \"decoder.lm.lm_head\", \"lm_head\"]\n",
    "        for key_part in potential_keys:\n",
    "            actual_key_to_assign = None\n",
    "            for existing_key in device_map.keys():\n",
    "                if key_part in existing_key and existing_key.endswith(\"lm_head\"):\n",
    "                    actual_key_to_assign = existing_key\n",
    "                    break\n",
    "            if actual_key_to_assign:\n",
    "                device_map[actual_key_to_assign] = main_device_idx_for_lm_head\n",
    "                lm_head_assigned = True\n",
    "                break\n",
    "        if not lm_head_assigned:\n",
    "             print(f\"[경고] Device map에서 lm_head를 자동으로 찾지 못했습니다. 수동 조정이 필요할 수 있습니다.\")\n",
    "\n",
    "        target_device = f'cuda:{main_device_idx_for_lm_head}'\n",
    "        aux_device = target_device\n",
    "    else:\n",
    "        device_map = {\"\": \"cpu\"}\n",
    "        target_device = 'cpu'\n",
    "        aux_device = 'cpu'\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    with tqdm(total=1, desc=f\"Emu 모델 ({model_name_or_path}) 가중치 로딩 중\", unit=\"op\") as pbar:\n",
    "        emu_model_loaded = load_checkpoint_and_dispatch(\n",
    "            emu_model,\n",
    "            snapshot_path,\n",
    "            device_map=device_map \n",
    "        ).eval()\n",
    "        pbar.update(1)\n",
    "    print(f'Emu 모델 ({model_name_or_path}) 로드 완료!')\n",
    "\n",
    "    clap_model = load_clap_model(checkpoint_path=clap_checkpoint_path, device=aux_device)\n",
    "    print(f'CLAP 모델 로드 완료 (device: {aux_device})')\n",
    "\n",
    "    user_projector_model = Projector(\n",
    "        input_patch_dim=512,\n",
    "        num_input_patches=256,\n",
    "        output_seq_len=256,\n",
    "        output_embed_dim=1792,\n",
    "        projector_transformer_hidden_dim=768,\n",
    "        projector_num_transformer_layers=8,\n",
    "        projector_num_heads=8,\n",
    "        projector_dropout=0.1\n",
    "    ).to(aux_device)\n",
    "    #user_projector_model = Projector(input_dim=512, output_seq_len=256, output_embed_dim=1792, hidden_dim=2048, num_layers=8, num_heads=8, dropout=0.1).to(aux_device)\n",
    "    user_projector_model.load_state_dict(torch.load(projector_checkpoint_path, map_location=aux_device))\n",
    "    user_projector_model.eval()\n",
    "    print(f'사용자 Projector 모델 로드 완료 (가중치: {projector_checkpoint_path}, device: {aux_device})')\n",
    "    \n",
    "    print(\"모든 모델 로딩 완료.\")\n",
    "    return tokenizer, emu_model_loaded, clap_model, user_projector_model, target_device, aux_device, emu_model_loaded.config.vision_config['image_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Each Modality Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Preprocessing Audio for CLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_for_clap(audio_path, target_sr=48000, target_duration_sec=10):\n",
    "    try:\n",
    "        waveform, sr = librosa.load(audio_path, sr=None)\n",
    "        if sr != target_sr:\n",
    "            waveform = librosa.resample(waveform, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        target_length = target_sr * target_duration_sec\n",
    "        current_length = waveform.shape[0]\n",
    "\n",
    "        if current_length < target_length:\n",
    "            padding = target_length - current_length\n",
    "            waveform = np.pad(waveform, (0, padding), 'constant')\n",
    "        elif current_length > target_length:\n",
    "            waveform = waveform[:target_length]\n",
    "            \n",
    "        return {'waveform': torch.tensor(waveform, dtype=torch.float32).unsqueeze(0), 'sample_rate': target_sr}\n",
    "    except Exception as e:\n",
    "        print(f\"오디오 파일 처리 오류 {audio_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Preprocessing Image for CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emu의 이미지 정규화를 위한 상수 (modeling_emu.py의 prepare_image_input 참조)\n",
    "OPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "OPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "def preprocess_image_for_emu(image_pil: Image.Image, image_size: int):\n",
    "    \"\"\"PIL 이미지를 Emu의 visual encoder 입력 형식에 맞게 전처리합니다.\n",
    "    modeling_emu.py의 prepare_image_input 메소드와 유사하게 동작합니다.\n",
    "\n",
    "    Args:\n",
    "        image_pil: 전처리할 PIL 이미지 객체입니다.\n",
    "        image_size: Emu 모델의 vision_config에서 가져온 목표 이미지 크기 (정사각형 가정)입니다.\n",
    "\n",
    "    Returns:\n",
    "        전처리된 이미지 텐서 (배치 차원 포함)입니다.\n",
    "    \"\"\"\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Resize((image_size, image_size), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(OPENAI_DATASET_MEAN, OPENAI_DATASET_STD),\n",
    "        ]\n",
    "    )\n",
    "    # unsqueeze(0)을 통해 배치 차원 [1, C, H, W]를 추가합니다.\n",
    "    return transform(image_pil).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Function to Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Define Function to Return Modality Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_modality_embedding(input_type, file_path, emu_model, clap_model, user_projector_model, aux_device, emu_image_size):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"입력 파일 {file_path}를 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    intermediate_embedding = None\n",
    "\n",
    "    if input_type == 'image':\n",
    "        try:\n",
    "            image_pil = Image.open(file_path).convert('RGB')\n",
    "            pixel_values = preprocess_image_for_emu(image_pil, emu_image_size)\n",
    "            target_device = aux_device\n",
    "            \n",
    "            if callable(emu_model.dtype):\n",
    "                target_dtype = emu_model.dtype()\n",
    "            else:\n",
    "                target_dtype = emu_model.dtype\n",
    "\n",
    "            pixel_values = pixel_values.to(device=target_device, dtype=target_dtype)\n",
    "\n",
    "            if hasattr(emu_model, 'model') and hasattr(emu_model.model, 'encode_image') and callable(getattr(emu_model.model, 'encode_image')):\n",
    "                intermediate_embedding = emu_model.model.encode_image(pixel_values)\n",
    "            else:\n",
    "                error_msg = \"Emu 모델 또는 그 내부 'model' 객체에서 'encode_image' 메소드를 찾을 수 없습니다.\"\n",
    "                # (기존 오류 메시지 로직과 동일)\n",
    "                print(f\"오류: {error_msg}\")\n",
    "                raise RuntimeError(error_msg)\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"이미지 파일 {file_path} 처리 중 오류 (get_intermediate_modality_embedding): {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    elif input_type == 'audio':\n",
    "        try:\n",
    "            processed_audio_data = preprocess_audio_for_clap(file_path)\n",
    "            if processed_audio_data is None:\n",
    "                return None\n",
    "            \n",
    "            waveform = processed_audio_data['waveform'].to(aux_device) # CLAP 모델 디바이스로\n",
    "\n",
    "            with torch.no_grad():\n",
    "                audio_embeddings_clap = get_clap_intermediate_patch_embeddings(\n",
    "                    clap_model_instance=clap_model,\n",
    "                    audio_waveforms=waveform,\n",
    "                    device=aux_device\n",
    "                )\n",
    "                #audio_embeddings_clap = clap_model.get_audio_embedding_from_data(waveform, use_tensor=True)\n",
    "                if audio_embeddings_clap is None or audio_embeddings_clap.shape[0] == 0:\n",
    "                    print(\"CLAP 모델에서 오디오 임베딩을 추출하지 못했습니다.\")\n",
    "                    return None\n",
    "\n",
    "                projector_device = next(user_projector_model.parameters()).device\n",
    "                projector_dtype = next(user_projector_model.parameters()).dtype\n",
    "\n",
    "                intermediate_embedding = user_projector_model(\n",
    "                    audio_embeddings_clap.to(device=projector_device, dtype=projector_dtype)\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"오디오 파일 {file_path} 처리 중 오류 (get_intermediate_modality_embedding): {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"지원되지 않는 입력 타입: {input_type}\")\n",
    "        return None\n",
    "\n",
    "    return intermediate_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(input_type, file_path, query, max_new_tokens, length_penalty, tokenizer, emu_model, clap_model, projector_model, emu_target_device, aux_device, emu_image_size):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"입력 파일 {file_path}를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    query_text = query\n",
    "    outputs = None \n",
    "\n",
    "    with torch.no_grad(): # 모든 연산을 no_grad 컨텍스트에서 수행\n",
    "        try:\n",
    "            intermediate_modality_embed = get_intermediate_modality_embedding(\n",
    "                input_type,\n",
    "                file_path,\n",
    "                emu_model,\n",
    "                clap_model,\n",
    "                projector_model,\n",
    "                aux_device,\n",
    "                emu_image_size\n",
    "            )\n",
    "\n",
    "            if intermediate_modality_embed is None:\n",
    "                        print(\"중간 모달리티 임베딩 추출 실패.\")\n",
    "                        return\n",
    "\n",
    "            modality_features_projected = emu_model.project_up(\n",
    "                intermediate_modality_embed.to(device=emu_model.project_up.weight.device, dtype=emu_model.dtype())\n",
    "            )\n",
    "            \n",
    "            '''\n",
    "            original_squeezed = intermediate_modality_embed.squeeze(0)  # shape: [256, 1792]\n",
    "            projected_squeezed = modality_features_projected.squeeze(0) # shape: [256, 6656]\n",
    "            \n",
    "            time_index = 51\n",
    "            original_vector = original_squeezed[time_index]\n",
    "            projected_vector = projected_squeezed[time_index]\n",
    "\n",
    "            print(f\"[Time Index {time_index}] Projected vector L2 norm: {projected_vector.norm().item():.4f}\")\n",
    "            print(f\"[Time Index {time_index}] Original vector L2 norm: {original_vector.norm().item():.4f}\")\n",
    "            \n",
    "            feature_index = 179\n",
    "            original_time_vector = original_squeezed[:, feature_index]\n",
    "            projected_time_vector = projected_squeezed[:, feature_index]\n",
    "\n",
    "            print(f\"[Feature Index {feature_index}] Projected time-sequence vector L2 norm: {projected_time_vector.norm().item():.4f}\")\n",
    "            print(f\"[Feature Index {feature_index}] Original time-sequence vector L2 norm: {original_time_vector.norm().item():.4f}\")\n",
    "            '''\n",
    "            \n",
    "            if not query_text:\n",
    "                query_text = '[<IMG_PLH>]Describe the image in details:' if input_type == 'image' else '[<IMG_PLH>]Describe the audio in details:'\n",
    "            \n",
    "            inputs_for_text = emu_model.build_input_ids(\n",
    "                text=[query_text],\n",
    "                tokenizer=tokenizer,\n",
    "                image=None \n",
    "            )\n",
    "            \n",
    "            input_ids = inputs_for_text[\"input_ids\"].to(emu_target_device)\n",
    "            attention_mask = inputs_for_text[\"attention_mask\"].to(emu_target_device)\n",
    "\n",
    "            text_embedding_layer = emu_model.model.decoder.lm.model.embed_tokens\n",
    "            text_embeds = text_embedding_layer(input_ids.to(text_embedding_layer.weight.device))\n",
    "            \n",
    "            DEFAULT_IMAGE_TOKEN = \"<image>\" \n",
    "            image_token_id = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "            n_query = emu_model.n_query\n",
    "            batch_idx = 0\n",
    "            actual_image_token_indices = (input_ids[batch_idx] == image_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            if len(actual_image_token_indices) > 0:\n",
    "                single_modality_feature_sequence = modality_features_projected.squeeze(0)\n",
    "                \n",
    "                num_tokens_to_replace = min(len(actual_image_token_indices), n_query, single_modality_feature_sequence.shape[0])\n",
    "                if len(actual_image_token_indices) != n_query or single_modality_feature_sequence.shape[0] != n_query:\n",
    "                    print(f\"경고 ({input_type}): <image> 토큰 수({len(actual_image_token_indices)}) 또는 피쳐 시퀀스 길이({single_modality_feature_sequence.shape[0]})가 n_query({n_query})와 다릅니다. {num_tokens_to_replace}개 주입.\")\n",
    "                \n",
    "                for i in range(num_tokens_to_replace):\n",
    "                    token_idx_in_sequence = actual_image_token_indices[i]\n",
    "                    text_embeds[batch_idx, token_idx_in_sequence, :] = single_modality_feature_sequence[i, :].to(text_embeds.device)\n",
    "            else:\n",
    "                raise RuntimeError(f\"오류 ({input_type}): <image> 토큰을 찾지 못하여 특징을 주입할 수 없습니다.\")\n",
    "            \n",
    "            generation_params = {\n",
    "                \"inputs_embeds\": text_embeds,\n",
    "                \"attention_mask\": attention_mask.to(text_embeds.device), # attention_mask도 text_embeds와 같은 장치로\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"length_penalty\": length_penalty,\n",
    "                \"num_beams\": 5,\n",
    "                \"min_length\": 1,\n",
    "                \"do_sample\": False,\n",
    "                \"penalty_alpha\": None,\n",
    "                \"top_p\": None,\n",
    "                \"top_k\": None,\n",
    "                \"temperature\": None,\n",
    "                \"repetition_penalty\": 1.0,\n",
    "            }\n",
    "            outputs = emu_model.model.decoder.lm.generate(**generation_params)\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"{input_type} 처리 또는 생성 중 오류 발생: {e}\")\n",
    "            traceback.print_exc()\n",
    "            outputs = None\n",
    "\n",
    "    if outputs is None:\n",
    "        print(\"텍스트 생성에 실패했습니다 (outputs가 None입니다).\")\n",
    "        return\n",
    "\n",
    "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'emu2chat' 모델 로딩 시작...\n",
      "Tokenizer (BAAI/Emu2-Chat) 로드 완료.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936a2638c13545269c15d93f296f934c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emu 모델 (BAAI/Emu2-Chat) 구조 로드 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emu 모델 (BAAI/Emu2-Chat) 가중치 로딩 중: 100%|██████████| 1/1 [02:04<00:00, 124.61s/op]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emu 모델 (BAAI/Emu2-Chat) 로드 완료!\n",
      "Load the specified checkpoint ./music_speech_audioset_epoch_15_esc_89.98.pt from users.\n",
      "Load Checkpoint...\n",
      "CLAP audio model loaded from ./music_speech_audioset_epoch_15_esc_89.98.pt and moved to cuda:1.\n",
      "CLAP 모델 로드 완료 (device: cuda:1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongmin/anaconda3/envs/emu2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 Projector 모델 로드 완료 (가중치: /home/jongmin/reference/Emu/Emu2/checkpoint/exp_projector_transformer/projector_epoch_028.pt, device: cuda:1)\n",
      "모든 모델 로딩 완료.\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE_TO_LOAD = 'emu2chat'\n",
    "PROJECTOR_CHECKPOINT_PATH_FOR_LOAD = '/home/jongmin/reference/Emu/Emu2/checkpoint/exp_projector_transformer/projector_epoch_028.pt'\n",
    "CLAP_CHECKPOINT_PATH_FOR_LOAD = './music_speech_audioset_epoch_15_esc_89.98.pt'\n",
    "\n",
    "tokenizer_instance, emu_core_instance, clap_instance, user_projector_instance, llm_device_instance, aux_device_instance, emu_image_size_instance = load_models(\n",
    "    MODEL_TYPE_TO_LOAD,\n",
    "    PROJECTOR_CHECKPOINT_PATH_FOR_LOAD,\n",
    "    CLAP_CHECKPOINT_PATH_FOR_LOAD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 실행 시작: 입력 타입='audio', 파일='/mnt/lynx1/datasets/places205/wavs/238/utterance_60731.wav'\n",
      "A river running through a lush green forest.\n"
     ]
    }
   ],
   "source": [
    "INPUT_TYPE_INFERENCE = 'audio'  # 'image' 또는 'audio'\n",
    "FILE_PATH_INFERENCE = '/mnt/lynx1/datasets/places205/wavs/238/utterance_60731.wav'# '/home/jongmin/reference/Emu/Emu2/examples/aud_example.wav' # '/mnt/lynx1/datasets/places205/wavs/238/utterance_60731.wav'\n",
    "#FILE_PATH_INFERENCE = '/home/jongmin/reference/Emu/Emu2/examples/img_example.jpg' # '/home/jongmin/reference/Emu/Emu2/examples/aud_example.wav'\n",
    "QUERY_TEXT_INFERENCE = '[<IMG_PLH>] Explain specifically what you can see in the image:'\n",
    "\n",
    "MAX_NEW_TOKENS_INFERENCE = 64\n",
    "LENGTH_PENALTY_INFERENCE = -1.0\n",
    "\n",
    "print(f\"추론 실행 시작: 입력 타입='{INPUT_TYPE_INFERENCE}', 파일='{FILE_PATH_INFERENCE}'\")\n",
    "# 이전에 정의된 run_inference 함수와 로드된 모델을 사용합니다.\n",
    "output = run_inference(\n",
    "    input_type=INPUT_TYPE_INFERENCE,\n",
    "    file_path=FILE_PATH_INFERENCE,\n",
    "    query=QUERY_TEXT_INFERENCE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS_INFERENCE,\n",
    "    length_penalty=LENGTH_PENALTY_INFERENCE,\n",
    "    tokenizer=tokenizer_instance,\n",
    "    emu_model=emu_core_instance,\n",
    "    clap_model=clap_instance,\n",
    "    projector_model=user_projector_instance,\n",
    "    emu_target_device=llm_device_instance,\n",
    "    aux_device=aux_device_instance,\n",
    "    emu_image_size=emu_image_size_instance\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data_and_save_results(\n",
    "    test_json_path,\n",
    "    result_json_path,\n",
    "    audio_base_path,\n",
    "    input_type,\n",
    "    query_text,\n",
    "    max_new_tokens,\n",
    "    length_penalty,\n",
    "    tokenizer,\n",
    "    emu_model,\n",
    "    clap_model,\n",
    "    projector_model,\n",
    "    emu_target_device,\n",
    "    aux_device,\n",
    "    emu_image_size\n",
    "):\n",
    "    try:\n",
    "        with open(test_json_path, 'r', encoding='utf-8') as f:\n",
    "            test_samples = json.load(f).get('data')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{test_json_path}' 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"오류: '{test_json_path}' 파일을 파싱하는 중 오류가 발생했습니다.\")\n",
    "        return\n",
    "\n",
    "    if not all(isinstance(s, dict) for s in test_samples):\n",
    "        print(f\"오류: '{test_json_path}' 파일의 형식이 잘못되었습니다. 각 항목은 딕셔너리여야 합니다.\")\n",
    "        return\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    print(f\"'{test_json_path}'에서 샘플을 읽어 추론을 시작합니다...\")\n",
    "\n",
    "    for sample in tqdm(test_samples, desc=\"추론 진행\"):\n",
    "        wav_relative_path = sample.get('wav')\n",
    "        if not wav_relative_path:\n",
    "            print(f\"경고: 샘플에 'wav' 키가 없습니다. 건너<0xEB><0x9B><0x84>니다: {sample}\")\n",
    "            output_text_result = \"Error: 'wav' key missing in sample\"\n",
    "        elif not isinstance(wav_relative_path, str):\n",
    "            print(f\"경고: 'wav' 경로가 문자열이 아닙니다. 건너<0xEB><0x9B><0x84>니다: {sample}\")\n",
    "            output_text_result = \"Error: 'wav' path is not a string\"\n",
    "        else:\n",
    "            file_path_inference = os.path.join(audio_base_path, wav_relative_path)\n",
    "\n",
    "            if not os.path.exists(file_path_inference):\n",
    "                print(f\"경고: 오디오 파일을 찾을 수 없습니다: {file_path_inference}. 건너<0xEB><0x9B><0x84>니다.\")\n",
    "                output_text_result = f\"Error: Audio file not found at {file_path_inference}\"\n",
    "            else:\n",
    "                # eval_main.ipynb에 정의된 run_inference 함수 호출\n",
    "                # 이 함수는 추론된 텍스트를 반환하도록 수정되어야 합니다.\n",
    "                try:\n",
    "                    # run_inference 함수가 정의된 노트북 셀이 실행되어 있어야 합니다.\n",
    "                    output_text_result = run_inference(\n",
    "                        input_type=input_type,\n",
    "                        file_path=file_path_inference,\n",
    "                        query=query_text,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        length_penalty=length_penalty,\n",
    "                        tokenizer=tokenizer,\n",
    "                        emu_model=emu_model,\n",
    "                        clap_model=clap_model,\n",
    "                        projector_model=projector_model,\n",
    "                        emu_target_device=emu_target_device,\n",
    "                        aux_device=aux_device,\n",
    "                        emu_image_size=emu_image_size\n",
    "                    )\n",
    "                    if output_text_result is None: # run_inference 내부에서 오류 발생 시 None 반환 가정\n",
    "                        output_text_result = \"Error: Inference failed (run_inference returned None)\"\n",
    "                        print(f\"추론 실패 (run_inference가 None을 반환) 파일: {file_path_inference}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"run_inference 실행 중 오류 발생 (파일: {file_path_inference}): {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    output_text_result = f\"Error: Exception during inference - {str(e)}\"\n",
    "\n",
    "        # 결과 저장 준비\n",
    "        current_result = sample.copy() # 기존 샘플 정보 복사\n",
    "        current_result['output_text'] = output_text_result\n",
    "        results_data.append(current_result)\n",
    "\n",
    "    try:\n",
    "        with open(result_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"추론 결과가 '{result_json_path}' 파일에 성공적으로 저장되었습니다.\")\n",
    "    except IOError:\n",
    "        print(f\"오류: '{result_json_path}' 파일을 쓰는 중 오류가 발생했습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과 저장 중 예기치 않은 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'test.json'에서 샘플을 읽어 추론을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "추론 진행: 100%|██████████| 100/100 [04:42<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 결과가 'result_ver2_epoch028.json' 파일에 성공적으로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "AUDIO_BASE_PATH = '/mnt/lynx1/datasets/places205/'\n",
    "IMAGE_BASE_PATH = '/mnt/lynx1/datasets/places205/vision/torralba/deeplearning/images256/'\n",
    "TEST_JSON_PATH = 'test.json'\n",
    "RESULT_JSON_PATH = 'result_ver2_epoch028.json'\n",
    "\n",
    "INPUT_TYPE_INFERENCE = 'audio'\n",
    "QUERY_TEXT_INFERENCE = '[<IMG_PLH>] Describe the image in details:'\n",
    "MAX_NEW_TOKENS_INFERENCE = 64\n",
    "LENGTH_PENALTY_INFERENCE = -1.0\n",
    "\n",
    "process_test_data_and_save_results(\n",
    "    TEST_JSON_PATH,\n",
    "    RESULT_JSON_PATH,\n",
    "    AUDIO_BASE_PATH,\n",
    "    INPUT_TYPE_INFERENCE,\n",
    "    QUERY_TEXT_INFERENCE,\n",
    "    MAX_NEW_TOKENS_INFERENCE,\n",
    "    LENGTH_PENALTY_INFERENCE,\n",
    "    tokenizer_instance,\n",
    "    emu_core_instance,\n",
    "    clap_instance,\n",
    "    user_projector_instance,\n",
    "    llm_device_instance,\n",
    "    aux_device_instance,\n",
    "    emu_image_size_instance\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_JSON_TO_LOAD = 'result.json'\n",
    "AUDIO_BASE_PATH_FOR_DISPLAY = '/mnt/lynx1/datasets/places205/'\n",
    "IMAGE_BASE_PATH_FOR_DISPLAY = '/mnt/lynx1/datasets/places205/vision/torralba/deeplearning/images256/'\n",
    "\n",
    "def display_result_by_index(result_file_path, index_to_display, audio_base, image_base):\n",
    "    try:\n",
    "        with open(result_file_path, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{result_file_path}' 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"오류: '{result_file_path}' 파일을 파싱하는 중 오류가 발생했습니다.\")\n",
    "        return\n",
    "\n",
    "    if not isinstance(results, list) or not results:\n",
    "        print(f\"'{result_file_path}' 파일이 비어있거나 잘못된 형식입니다.\")\n",
    "        return\n",
    "\n",
    "    # 인덱스는 1부터 시작한다고 가정 (사용자 요청)\n",
    "    actual_index = index_to_display - 1\n",
    "\n",
    "    if not 0 <= actual_index < len(results):\n",
    "        print(f\"오류: 인덱스 {index_to_display}는 유효한 범위(1 ~ {len(results)})를 벗어났습니다.\")\n",
    "        return\n",
    "\n",
    "    data_item = results[actual_index]\n",
    "\n",
    "    print(f\"--- 데이터 인덱스: {index_to_display} ---\")\n",
    "\n",
    "    # 이미지 로드 및 출력\n",
    "    image_relative_path = data_item.get('image')\n",
    "    if image_relative_path and isinstance(image_relative_path, str):\n",
    "        image_full_path = os.path.join(image_base, image_relative_path)\n",
    "        try:\n",
    "            if os.path.exists(image_full_path):\n",
    "                img = Image.open(image_full_path)\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Image: {os.path.basename(image_full_path)}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"이미지 파일을 찾을 수 없습니다: {image_full_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"이미지 로드 중 오류 발생 ({image_full_path}): {e}\")\n",
    "    elif 'image' not in data_item:\n",
    "        print(\"데이터에 'image' 키가 없습니다.\")\n",
    "    else:\n",
    "        print(f\"'image' 경로가 유효하지 않습니다: {image_relative_path}\")\n",
    "\n",
    "\n",
    "    # 오디오 로드 및 출력 (재생 위젯)\n",
    "    audio_relative_path = data_item.get('wav')\n",
    "    if audio_relative_path and isinstance(audio_relative_path, str):\n",
    "        audio_full_path = os.path.join(audio_base, audio_relative_path)\n",
    "        try:\n",
    "            if os.path.exists(audio_full_path):\n",
    "                print(f\"Audio: {os.path.basename(audio_full_path)}\")\n",
    "                ipd.display(ipd.Audio(audio_full_path))\n",
    "            else:\n",
    "                print(f\"오디오 파일을 찾을 수 없습니다: {audio_full_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"오디오 로드/재생 중 오류 발생 ({audio_full_path}): {e}\")\n",
    "    elif 'wav' not in data_item:\n",
    "        print(\"데이터에 'wav' 키가 없습니다.\")\n",
    "    else:\n",
    "        print(f\"'wav' 경로가 유효하지 않습니다: {audio_relative_path}\")\n",
    "\n",
    "    # ASR 텍스트와 Output 텍스트 비교 출력\n",
    "    asr_text = data_item.get('asr_text', \"N/A (asr_text 없음)\")\n",
    "    output_text = data_item.get('output_text', \"N/A (output_text 없음)\")\n",
    "\n",
    "    print(f\"\\nASR Text (원문):\")\n",
    "    print(f\"  {asr_text}\")\n",
    "    print(f\"\\nOutput Text (추론 결과):\")\n",
    "    print(f\"  {output_text}\")\n",
    "    print(\"--- --- ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result_by_index(RESULT_JSON_TO_LOAD, 3, AUDIO_BASE_PATH_FOR_DISPLAY, IMAGE_BASE_PATH_FOR_DISPLAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Comparing Two Modality Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback # 오류 로깅용\n",
    "\n",
    "JSON_FILE_PATH = '/home/jongmin/reference/Emu/Emu2/train1k.json' \n",
    "\n",
    "DEFAULT_AUDIO_BASE_PATH = '/mnt/lynx1/datasets/places205/'\n",
    "DEFAULT_IMAGE_BASE_PATH = '/mnt/lynx1/datasets/places205/vision/torralba/deeplearning/images256/'\n",
    "\n",
    "audio_emb_list = []\n",
    "image_emb_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Computing Two Modality Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(JSON_FILE_PATH):\n",
    "    print(f\"오류: JSON 파일 '{JSON_FILE_PATH}'을(를) 찾을 수 없습니다. 스크립트를 종료합니다.\")\n",
    "else:\n",
    "    with open(JSON_FILE_PATH, 'r') as f:\n",
    "        dataset_json_content = json.load(f)\n",
    "\n",
    "    data_entries = dataset_json_content.get('data', [])\n",
    "    current_audio_base_path = DEFAULT_AUDIO_BASE_PATH\n",
    "    current_image_base_path = DEFAULT_IMAGE_BASE_PATH\n",
    "\n",
    "    if 'emu_core_instance' in globals(): emu_core_instance.eval()\n",
    "    if 'clap_instance' in globals(): clap_instance.eval()\n",
    "    if 'user_projector_instance' in globals(): user_projector_instance.eval()\n",
    "\n",
    "    for entry in tqdm(data_entries, desc=\"데이터셋 임베딩 추출 중\"):\n",
    "        wav_relative_path = entry.get('wav')\n",
    "        image_relative_path = entry.get('image')\n",
    "\n",
    "        audio_full_path = os.path.join(current_audio_base_path, wav_relative_path)\n",
    "        image_full_path = os.path.join(current_image_base_path, image_relative_path)\n",
    "\n",
    "        # --- 이미지 임베딩 추출 ---\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "               img_intermediate_embed = get_intermediate_modality_embedding(\n",
    "                    input_type='image',\n",
    "                    file_path=image_full_path,\n",
    "                    emu_model=emu_core_instance, \n",
    "                    clap_model=None, \n",
    "                    user_projector_model=None, \n",
    "                    aux_device=aux_device_instance,\n",
    "                    emu_image_size=emu_image_size_instance \n",
    "                )\n",
    "            \n",
    "            if img_intermediate_embed is not None:\n",
    "                #img_embedding_to_store = img_intermediate_embed.mean(dim=1).squeeze(0) # 결과: [C_visual] (예: [1792])\n",
    "                img_embedding_to_store = torch.max(img_intermediate_embed, dim=1).values.squeeze(0)\n",
    "                image_emb_list.append(img_embedding_to_store.detach().cpu())\n",
    "                del img_intermediate_embed, img_embedding_to_store\n",
    "            else:\n",
    "                print(f\"이미지 중간 임베딩 추출 실패: {image_full_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"이미지 처리 중 오류 발생 ({image_full_path}): {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # --- 오디오 임베딩 추출 ---\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                audio_intermediate_embed = get_intermediate_modality_embedding(\n",
    "                    input_type='audio',\n",
    "                    file_path=audio_full_path,\n",
    "                    emu_model=None,\n",
    "                    clap_model=clap_instance,\n",
    "                    user_projector_model=user_projector_instance,\n",
    "                    aux_device=aux_device_instance,\n",
    "                    emu_image_size=None\n",
    "                )\n",
    "\n",
    "            if audio_intermediate_embed is not None:\n",
    "                #audio_embedding_to_store = audio_intermediate_embed.mean(dim=1).squeeze(0)\n",
    "                audio_embedding_to_store = torch.max(audio_intermediate_embed, dim=1).values.squeeze(0)\n",
    "                audio_emb_list.append(audio_embedding_to_store.detach().cpu())\n",
    "                del audio_intermediate_embed, audio_embedding_to_store\n",
    "            else:\n",
    "                print(f\"오디오 중간 임베딩 추출 실패: {audio_full_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"오디오 처리 중 오류 발생 ({audio_full_path}): {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n--- 임베딩 추출 완료 ---\")\n",
    "print(f\"총 추출된 이미지 임베딩 수: {len(image_emb_list)}\")\n",
    "print(f\"총 추출된 오디오 임베딩 수: {len(audio_emb_list)}\")\n",
    "\n",
    "if image_emb_list:\n",
    "    print(f\"첫 번째 이미지 임베딩 형태: {image_emb_list[0].shape}, dtype: {image_emb_list[0].dtype} (on CPU)\")\n",
    "else:\n",
    "    print(\"이미지 임베딩이 추출되지 않았습니다.\")\n",
    "\n",
    "if audio_emb_list:\n",
    "    print(f\"첫 번째 오디오 임베딩 형태: {audio_emb_list[0].shape}, dtype: {audio_emb_list[0].dtype} (on CPU)\")\n",
    "else:\n",
    "    print(\"오디오 임베딩이 추출되지 않았습니다.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings_tensor = torch.stack(audio_emb_list, dim=0)\n",
    "image_embeddings_tensor = torch.stack(image_emb_list, dim=0)\n",
    "\n",
    "print(audio_embeddings_tensor.shape)\n",
    "print(image_embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from DOSNES.dosnes.dosnes import DOSNES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.concatenate([\n",
    "    audio_embeddings_tensor.numpy(), \n",
    "    image_embeddings_tensor.to(torch.float32).numpy()\n",
    "], axis=0)\n",
    "\n",
    "labels = np.array([0] * len(audio_embeddings_tensor) + [1] * len(image_embeddings_tensor))\n",
    "\n",
    "dosnes = DOSNES(metric=\"cosine\", verbose=1, random_state=42)\n",
    "embedded = dosnes.fit_transform(all_embeddings, y=labels, filename=\"dosnes_result_CS_Divergence.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio_emb_list)\n",
    "print(image_emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = image_emb_list[10]\n",
    "b = audio_emb_list[10]\n",
    "print(torch.dot(a, a))\n",
    "print(torch.dot(b, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clap_instance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
